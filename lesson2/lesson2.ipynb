{
 "cells": [
  {
   "cell_type": "raw",
   "id": "continental-blogger",
   "metadata": {},
   "source": [
    "1. *Самостоятельно повторить tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "2. Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог)\n",
    "\n",
    "3. Повторить п.2, но используя уже не медиану, а max\n",
    "\n",
    "4. *Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (взяв список новостей пользователя)\n",
    "подсказка 1: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал\n",
    "подсказка 2: нужен именно idf, как вес.\n",
    "\n",
    "5. Сформировать на выходе единую таблицу, сравнивающую качество 2/3 разных метода получения эмбедингов пользователей: median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "\n",
    "6. Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unique-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, precision_score,\n",
    "                             classification_report, precision_recall_curve, confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satisfactory-yeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-09 09:19:56--  https://docs.google.com/uc?export=download&confirm=t&id=1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR\n",
      "Resolving docs.google.com (docs.google.com)... 64.233.162.194, 2a00:1450:4010:c05::c2\n",
      "Connecting to docs.google.com (docs.google.com)|64.233.162.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0hjaktjs921ih2e97cvtcuchc0d6c1q7/1649485125000/14904333240138417226/*/1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-04-09 09:19:57--  https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0hjaktjs921ih2e97cvtcuchc0d6c1q7/1649485125000/14904333240138417226/*/1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR?e=download\n",
      "Resolving doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)... 64.233.165.132, 2a00:1450:4010:c08::84\n",
      "Connecting to doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)|64.233.165.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 145159860 (138M) [text/csv]\n",
      "Saving to: ‘articles.csv’\n",
      "\n",
      "articles.csv        100%[===================>] 138,43M  10,8MB/s    in 13s     \n",
      "\n",
      "2022-04-09 09:20:10 (10,9 MB/s) - ‘articles.csv’ saved [145159860/145159860]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR\" -O articles.csv && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heavy-platform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-09 09:20:10--  https://drive.google.com/uc?export=download&id=1Q97K9eGrvpbS4ut5CphZa--gJDRqQp2a\n",
      "Resolving drive.google.com (drive.google.com)... 64.233.162.194, 2a00:1450:4010:c05::c2\n",
      "Connecting to drive.google.com (drive.google.com)|64.233.162.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-04-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/708a252vqkv6laqj4q2989v1b59cod09/1649485200000/14904333240138417226/*/1Q97K9eGrvpbS4ut5CphZa--gJDRqQp2a?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-04-09 09:20:11--  https://doc-04-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/708a252vqkv6laqj4q2989v1b59cod09/1649485200000/14904333240138417226/*/1Q97K9eGrvpbS4ut5CphZa--gJDRqQp2a?e=download\n",
      "Resolving doc-04-c0-docs.googleusercontent.com (doc-04-c0-docs.googleusercontent.com)... 64.233.165.132, 2a00:1450:4010:c08::84\n",
      "Connecting to doc-04-c0-docs.googleusercontent.com (doc-04-c0-docs.googleusercontent.com)|64.233.165.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 434166 (424K) [text/csv]\n",
      "Saving to: ‘users_articles.csv’\n",
      "\n",
      "users_articles.csv  100%[===================>] 423,99K  --.-KB/s    in 0,07s   \n",
      "\n",
      "2022-04-09 09:20:11 (6,03 MB/s) - ‘users_articles.csv’ saved [434166/434166]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.google.com/uc?export=download&id=1Q97K9eGrvpbS4ut5CphZa--gJDRqQp2a' -O users_articles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "durable-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-09 09:20:11--  https://drive.google.com/uc?export=download&id=17wVn5YPpMjHToctGgff_KfSeWcIIlf7c\n",
      "Resolving drive.google.com (drive.google.com)... 64.233.162.194, 2a00:1450:4010:c05::c2\n",
      "Connecting to drive.google.com (drive.google.com)|64.233.162.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/e3vgkh5sc7uj96sp41i8rda5mesrrec5/1649485200000/14904333240138417226/*/17wVn5YPpMjHToctGgff_KfSeWcIIlf7c?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-04-09 09:20:12--  https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/e3vgkh5sc7uj96sp41i8rda5mesrrec5/1649485200000/14904333240138417226/*/17wVn5YPpMjHToctGgff_KfSeWcIIlf7c?e=download\n",
      "Resolving doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)... 64.233.165.132, 2a00:1450:4010:c08::84\n",
      "Connecting to doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)|64.233.165.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5368 (5,2K) [text/plain]\n",
      "Saving to: ‘stopwords.txt’\n",
      "\n",
      "stopwords.txt       100%[===================>]   5,24K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-04-09 09:20:12 (18,3 MB/s) - ‘stopwords.txt’ saved [5368/5368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.google.com/uc?export=download&id=17wVn5YPpMjHToctGgff_KfSeWcIIlf7c' -O stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modern-trust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-09 09:20:12--  https://drive.google.com/uc?export=download&id=1nWKteQgEr9Rl8CwTRY7N2f7igevNH7oK\n",
      "Resolving drive.google.com (drive.google.com)... 64.233.162.194, 2a00:1450:4010:c05::c2\n",
      "Connecting to drive.google.com (drive.google.com)|64.233.162.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/miah63uarsctseb7t02pnq67ca0ip86o/1649485200000/14904333240138417226/*/1nWKteQgEr9Rl8CwTRY7N2f7igevNH7oK?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-04-09 09:20:13--  https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/miah63uarsctseb7t02pnq67ca0ip86o/1649485200000/14904333240138417226/*/1nWKteQgEr9Rl8CwTRY7N2f7igevNH7oK?e=download\n",
      "Resolving doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)... 64.233.165.132, 2a00:1450:4010:c08::84\n",
      "Connecting to doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)|64.233.165.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 80010 (78K) [text/csv]\n",
      "Saving to: ‘users_churn.csv’\n",
      "\n",
      "users_churn.csv     100%[===================>]  78,13K  --.-KB/s    in 0,03s   \n",
      "\n",
      "2022-04-09 09:20:13 (2,60 MB/s) - ‘users_churn.csv’ saved [80010/80010]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.google.com/uc?export=download&id=1nWKteQgEr9Rl8CwTRY7N2f7igevNH7oK' -O users_churn.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amber-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"articles.csv\")\n",
    "users = pd.read_csv(\"users_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "answering-george",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/boormistr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopword_ru = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dependent-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "    \n",
    "stopword_ru += additional_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "macro-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    text = re.sub('n', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatization(text):    \n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист лемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w) > 1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords = [i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unlikely-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27000 [00:00<?, ?it/s]<ipython-input-9-898b573b2830>:14: FutureWarning: Possible nested set at position 39\n",
      "  text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
      "100%|██████████| 27000/27000 [00:20<00:00, 1340.90it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "news['title'] = news['title'].progress_apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ahead-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27000/27000 [02:11<00:00, 205.91it/s] \n"
     ]
    }
   ],
   "source": [
    "news['title'] = news['title'].progress_apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hourly-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(news['title'].values)\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "guilty-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_topic = 20\n",
    "lda = LdaModel(common_corpus, num_topics=N_topic, id2word=common_dictionary, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "impossible-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "timely-calibration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: сотрудник уголовный орган задержать лицо убийство данные\n",
      "topic_1: смерть высота родственник умереть горный бежать вниз\n",
      "topic_2: система земля первый ракета новый статья американский\n",
      "topic_3: тело взрыв дом квартира памятник французский франция\n",
      "topic_4: россия российский млрд глава эксперт государство рф\n",
      "topic_5: китай китайский северный япония японский южный рейтинг\n",
      "topic_6: проект научный газ новый фонд развитие программа\n",
      "topic_7: планета пенсия команда девочка игра физика мышь\n",
      "topic_8: москва первый сайт мероприятие день площадь известный\n",
      "topic_9: украина военный украинский сила армия территория российский\n",
      "topic_10: сша россия американский российский санкция европа новый\n",
      "topic_11: рубль млн рост рынок банк цена тыс\n",
      "topic_12: долг задолженность бесплатный подсчитать медик ter аналог\n",
      "topic_13: полоса брюссель парижский приморский чикаго непрерывный прочность\n",
      "topic_14: снижение белоруссия миссия иран восток сигнал запустить\n",
      "topic_15: гражданин россиянин россия артист таиланд российский казахстан\n",
      "topic_16: произойти день район погибнуть министерство обнаружить изз\n",
      "topic_17: всё очень день писать сделать газета большой\n",
      "topic_18: ребёнок исследование женщина жизнь возраст мужчина школа\n",
      "topic_19: закон решение совет глава депутат россия правительство\n"
     ]
    }
   ],
   "source": [
    "x = lda.show_topics(num_topics=N_topic, num_words=7, formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "# Печатаем только слова\n",
    "for topic, words in topics_words:\n",
    "    print(f\"topic_{topic}: \" + \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "floral-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_vector(lda, text):\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(N_topic):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adjustable-reward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087853</td>\n",
       "      <td>0.112566</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030749</td>\n",
       "      <td>0.031307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.147673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.604702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130804</td>\n",
       "      <td>0.369308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179173</td>\n",
       "      <td>0.072669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id   topic_0   topic_1   topic_2   topic_3   topic_4  topic_5  \\\n",
       "0       6  0.000000  0.000000  0.000000  0.000000  0.013482      0.0   \n",
       "1    4896  0.147673  0.000000  0.000000  0.175095  0.000000      0.0   \n",
       "2    4897  0.000000  0.058081  0.000000  0.000000  0.000000      0.0   \n",
       "3    4898  0.000000  0.000000  0.095446  0.000000  0.000000      0.0   \n",
       "4    4899  0.000000  0.000000  0.000000  0.000000  0.000000      0.0   \n",
       "\n",
       "    topic_6   topic_7   topic_8  ...  topic_10  topic_11  topic_12  topic_13  \\\n",
       "0  0.087853  0.112566  0.010342  ...  0.030749  0.031307       0.0  0.000000   \n",
       "1  0.000000  0.434370  0.000000  ...  0.000000  0.000000       0.0  0.000000   \n",
       "2  0.000000  0.604702  0.000000  ...  0.000000  0.033123       0.0  0.000000   \n",
       "3  0.000000  0.307694  0.000000  ...  0.000000  0.051072       0.0  0.000000   \n",
       "4  0.000000  0.179173  0.072669  ...  0.222622  0.000000       0.0  0.030978   \n",
       "\n",
       "   topic_14  topic_15  topic_16  topic_17  topic_18  topic_19  \n",
       "0  0.010443       0.0  0.000000  0.000000       0.0  0.697176  \n",
       "1  0.000000       0.0  0.108574  0.000000       0.0  0.000000  \n",
       "2  0.000000       0.0  0.000000  0.283005       0.0  0.000000  \n",
       "3  0.012821       0.0  0.130804  0.369308       0.0  0.024501  \n",
       "4  0.000000       0.0  0.000000  0.000000       0.0  0.472426  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(lda, text) for text in news['title'].values])\n",
    "topic_matrix.columns = [f'topic_{i}' for i in range(N_topic)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+[f'topic_{i}' for i in range(N_topic)]]\n",
    "topic_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "looking-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[[f'topic_{i}' for i in range(N_topic)]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conservative-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([['F-Score'], ['Precision'], ['Recall'], ['ROC AUC']], columns=['Metric'])\n",
    "results = results.set_index('Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "collective-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"users_churn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-parker",
   "metadata": {},
   "source": [
    "### Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "turkish-basket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F-Score</th>\n",
       "      <td>0.800766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.754513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.853061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.974536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Max\n",
       "Metric             \n",
       "F-Score    0.800766\n",
       "Precision  0.754513\n",
       "Recall     0.853061\n",
       "ROC AUC    0.974536"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_user_embedding(user_articles_list, doc_dict):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = np.max(user_vector, 0)\n",
    "    return user_vector\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, doc_dict))])\n",
    "user_embeddings.columns = [f'topic_{i}' for i in range(N_topic)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+[f'topic_{i}' for i in range(N_topic)]]\n",
    "user_embeddings.head(10)\n",
    "\n",
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[[f'topic_{i}' for i in range(N_topic)]], \n",
    "                                                   X['churn'], random_state=0)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, preds)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "ix = np.argmax(fscore)\n",
    "\n",
    "results['Max'] = [fscore[ix], precision[ix], recall[ix], auc]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-estonia",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mineral-small",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F-Score</th>\n",
       "      <td>0.800766</td>\n",
       "      <td>0.804781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.754513</td>\n",
       "      <td>0.785992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.853061</td>\n",
       "      <td>0.824490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.974536</td>\n",
       "      <td>0.974517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Max    Median\n",
       "Metric                       \n",
       "F-Score    0.800766  0.804781\n",
       "Precision  0.754513  0.785992\n",
       "Recall     0.853061  0.824490\n",
       "ROC AUC    0.974536  0.974517"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_user_embedding(user_articles_list, doc_dict):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = np.median(user_vector, 0)\n",
    "    return user_vector\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, doc_dict))])\n",
    "user_embeddings.columns = [f'topic_{i}' for i in range(N_topic)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+[f'topic_{i}' for i in range(N_topic)]]\n",
    "user_embeddings.head(10)\n",
    "\n",
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[[f'topic_{i}' for i in range(N_topic)]], \n",
    "                                                    X['churn'], random_state=0)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, preds)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "ix = np.argmax(fscore)\n",
    "\n",
    "results['Median'] = [fscore[ix], precision[ix], recall[ix], auc]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-bahrain",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "searching-chosen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(users['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "figured-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = pd.DataFrame({'article_id': tfidf.get_feature_names_out(),\n",
    "                  'idf': tfidf.idf_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "identified-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max</th>\n",
       "      <th>Median</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F-Score</th>\n",
       "      <td>0.800766</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.884696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.754513</td>\n",
       "      <td>0.785992</td>\n",
       "      <td>0.909483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.853061</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.861224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC AUC</th>\n",
       "      <td>0.974536</td>\n",
       "      <td>0.974517</td>\n",
       "      <td>0.987690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Max    Median     tfidf\n",
       "Metric                                 \n",
       "F-Score    0.800766  0.804781  0.884696\n",
       "Precision  0.754513  0.785992  0.909483\n",
       "Recall     0.853061  0.824490  0.861224\n",
       "ROC AUC    0.974536  0.974517  0.987690"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_user_embedding(user_articles_list, doc_dict):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    \n",
    "    user_vector = np.zeros((len(user_articles_list), N_topic))\n",
    "    for i, doc_id in enumerate(user_articles_list):\n",
    "        try:\n",
    "            weight = idf[idf['article_id'] == str(doc_id)]['idf'].values[0]\n",
    "        except Exception as e:\n",
    "            weight = 0\n",
    "        user_vector[i] = doc_dict[doc_id] * weight\n",
    "    \n",
    "    user_vector = np.median(user_vector, axis=0)\n",
    "    return user_vector\n",
    "\n",
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, doc_dict))])\n",
    "user_embeddings.columns = [f'topic_{i}' for i in range(N_topic)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+[f'topic_{i}' for i in range(N_topic)]]\n",
    "user_embeddings.head(10)\n",
    "\n",
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[[f'topic_{i}' for i in range(N_topic)]], \n",
    "                                                    X['churn'], random_state=0)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, preds)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "ix = np.argmax(fscore)\n",
    "\n",
    "results['tfidf'] = [fscore[ix], precision[ix], recall[ix], auc]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-demonstration",
   "metadata": {},
   "source": [
    "Лучшим способом оказался tfidf по причине того, что в данном случае выделяются наиболее специализированные новости, которые лучше всего описывают пользователя."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
