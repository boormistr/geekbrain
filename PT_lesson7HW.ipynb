{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad69fff",
   "metadata": {},
   "source": [
    "Домашнее задание.\n",
    "1. Попробуйте обучить нейронную сеть с применением одномерных сверток для предсказания сентимента сообщений с твитера на примере https://www.kaggle.com/datasets/arkhoshghalb/twitter-sentiment-analysis-hatred-speech\n",
    "2. Опишите, какой результат вы получили? Что помогло вам улучшить ее точность?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fb74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4da7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "max_len = 20\n",
    "num_classes = 1\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 512\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9456c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('twitter_data/train.csv')\n",
    "df_test = pd.read_csv('twitter_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce914808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca23c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stop-words in /home/boormistr/.local/lib/python3.10/site-packages (2018.7.23)\n",
      "Requirement already satisfied: pymorphy2 in /home/boormistr/.local/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /home/boormistr/.local/lib/python3.10/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /home/boormistr/.local/lib/python3.10/site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in /home/boormistr/.local/lib/python3.10/site-packages (from pymorphy2) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b5cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce59cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb70e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b856b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in puncts)\n",
    "    txt = txt.lower()\n",
    "#     txt = re.sub(\"не\\s\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4552ad07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 22373/22373 [00:03<00:00, 6274.23it/s]\n",
      "100%|█████████████████████████████████████| 9589/9589 [00:01<00:00, 6708.74it/s]\n",
      "100%|███████████████████████████████████| 17197/17197 [00:02<00:00, 6452.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_train['tweet'] = df_train['tweet'].progress_apply(preprocess_text)\n",
    "df_val['tweet'] = df_val['tweet'].progress_apply(preprocess_text)\n",
    "df_test['tweet'] = df_test['tweet'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4ab2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = \" \".join(df_train[\"tweet\"])\n",
    "train_corpus = train_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663beef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/boormistr/.local/lib/python3.10/site-packages (3.8)\n",
      "Requirement already satisfied: click in /home/boormistr/.local/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/boormistr/.local/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/boormistr/.local/lib/python3.10/site-packages (from nltk) (2022.3.2)\n",
      "Requirement already satisfied: tqdm in /home/boormistr/.local/lib/python3.10/site-packages (from nltk) (4.64.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32824eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/boormistr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['user', 'one', 'awesome', 'displays', 'bbc']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "tokens = word_tokenize(train_corpus)\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f417178",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "582911ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "dist = FreqDist(tokens_filtered)\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]\n",
    "len(tokens_filtered_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85242c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dc3907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0]*(maxlen-len(result))\n",
    "    return result[-maxlen:] + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce369c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"tweet\"]])\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in df_test[\"tweet\"]])\n",
    "x_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"tweet\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97dfcfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bec99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size=2000, embedding_dim=128, hidden_dim=128, use_last=True):\n",
    "        super().__init__()\n",
    "        self.use_last = use_last\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        lstm_out, ht = self.lstm(x)\n",
    "        \n",
    "        if self.use_last:\n",
    "            last_tensor = lstm_out[:,-1,:]\n",
    "        else:\n",
    "            last_tensor = torch.mean(lstm_out[:,:], dim=1)\n",
    "        \n",
    "        out = self.linear(last_tensor)\n",
    "        \n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8239a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DataWrapper(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.target = torch.from_numpy(target)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bae06fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataWrapper(x_train, df_train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = DataWrapper(x_val, df_val['label'].values)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c5de042",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(max_words, 128, 128, use_last=False)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "756601de",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c48a5d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]. Step: [44/44]. Loss: 0.189, Acc: 0.93215. Test loss:  0.187.             Test acc:  0.94243\n",
      "Epoch [2/30]. Step: [44/44]. Loss: 0.168, Acc: 0.94677. Test loss:  0.170.             Test acc:  0.95046\n",
      "Epoch [3/30]. Step: [44/44]. Loss: 0.119, Acc: 0.95870. Test loss:  0.146.             Test acc:  0.95297\n",
      "Epoch [4/30]. Step: [44/44]. Loss: 0.098, Acc: 0.96473. Test loss:  0.135.             Test acc:  0.95245\n",
      "Epoch [5/30]. Step: [44/44]. Loss: 0.105, Acc: 0.97037. Test loss:  0.129.             Test acc:  0.95265\n",
      "Epoch [6/30]. Step: [44/44]. Loss: 0.085, Acc: 0.97269. Test loss:  0.146.             Test acc:  0.95015\n",
      "Epoch [7/30]. Step: [44/44]. Loss: 0.083, Acc: 0.97680. Test loss:  0.215.             Test acc:  0.94504\n",
      "Epoch [8/30]. Step: [44/44]. Loss: 0.061, Acc: 0.97904. Test loss:  0.181.             Test acc:  0.94713\n",
      "Epoch [9/30]. Step: [44/44]. Loss: 0.032, Acc: 0.98369. Test loss:  0.247.             Test acc:  0.95245\n",
      "Epoch [10/30]. Step: [44/44]. Loss: 0.042, Acc: 0.98458. Test loss:  0.246.             Test acc:  0.94973\n",
      "Epoch [11/30]. Step: [44/44]. Loss: 0.055, Acc: 0.98565. Test loss:  0.141.             Test acc:  0.94963\n",
      "Epoch [12/30]. Step: [44/44]. Loss: 0.042, Acc: 0.98802. Test loss:  0.283.             Test acc:  0.94932\n",
      "Epoch [13/30]. Step: [44/44]. Loss: 0.021, Acc: 0.99008. Test loss:  0.259.             Test acc:  0.94953\n",
      "Epoch [14/30]. Step: [44/44]. Loss: 0.043, Acc: 0.99093. Test loss:  0.215.             Test acc:  0.94921\n",
      "Epoch [15/30]. Step: [44/44]. Loss: 0.035, Acc: 0.99146. Test loss:  0.337.             Test acc:  0.95046\n",
      "Epoch [16/30]. Step: [44/44]. Loss: 0.043, Acc: 0.99169. Test loss:  0.357.             Test acc:  0.94942\n",
      "Epoch [17/30]. Step: [44/44]. Loss: 0.023, Acc: 0.99191. Test loss:  0.277.             Test acc:  0.94671\n",
      "Epoch [18/30]. Step: [44/44]. Loss: 0.025, Acc: 0.99240. Test loss:  0.316.             Test acc:  0.94452\n",
      "Epoch [19/30]. Step: [44/44]. Loss: 0.004, Acc: 0.99338. Test loss:  0.160.             Test acc:  0.94911\n",
      "Epoch [20/30]. Step: [44/44]. Loss: 0.017, Acc: 0.99289. Test loss:  0.231.             Test acc:  0.94754\n",
      "Epoch [21/30]. Step: [44/44]. Loss: 0.007, Acc: 0.99432. Test loss:  0.297.             Test acc:  0.94379\n",
      "Epoch [22/30]. Step: [44/44]. Loss: 0.008, Acc: 0.99316. Test loss:  0.269.             Test acc:  0.94807\n",
      "Epoch [23/30]. Step: [44/44]. Loss: 0.034, Acc: 0.99397. Test loss:  0.364.             Test acc:  0.94556\n",
      "Epoch [24/30]. Step: [44/44]. Loss: 0.008, Acc: 0.99446. Test loss:  0.350.             Test acc:  0.94525\n",
      "Epoch [25/30]. Step: [44/44]. Loss: 0.014, Acc: 0.99414. Test loss:  0.201.             Test acc:  0.94588\n",
      "Epoch [26/30]. Step: [44/44]. Loss: 0.009, Acc: 0.99450. Test loss:  0.350.             Test acc:  0.94546\n",
      "Epoch [27/30]. Step: [44/44]. Loss: 0.033, Acc: 0.99486. Test loss:  0.238.             Test acc:  0.94900\n",
      "Epoch [28/30]. Step: [44/44]. Loss: 0.025, Acc: 0.99437. Test loss:  0.511.             Test acc:  0.94567\n",
      "Epoch [29/30]. Step: [44/44]. Loss: 0.009, Acc: 0.99549. Test loss:  0.207.             Test acc:  0.94546\n",
      "Epoch [30/30]. Step: [44/44]. Loss: 0.010, Acc: 0.99522. Test loss:  0.231.             Test acc:  0.94848\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.train()\n",
    "th = 0.5\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_items, running_right = 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.item()\n",
    "        running_items += len(labels)\n",
    "        pred_labels = torch.squeeze((outputs > th).int())\n",
    "        running_right += (labels == pred_labels).sum()\n",
    "    \n",
    "    model.eval()\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}]. ' \\\n",
    "          f'Step: [{i + 1}/{len(train_loader)}]. '\\\n",
    "          f'Loss: {loss:.3f}, ' \\\n",
    "          f'Acc: {running_right / running_items:.5f}', end='. ')\n",
    "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "    train_loss_history.append(loss)\n",
    "    \n",
    "    test_running_right, test_running_total, test_loss = 0.0, 0.0, 0.0\n",
    "    for j, data in enumerate(val_loader):\n",
    "        test_labels = data[1].to(device)\n",
    "        test_outputs = model(data[0].to(device))\n",
    "        \n",
    "        test_loss = criterion(test_outputs, test_labels.float().view(-1,1))\n",
    "        test_running_total += len(data[1])\n",
    "        pred_test_labels = torch.squeeze((test_outputs > th).int())\n",
    "        test_running_right += (test_labels == pred_test_labels).sum()\n",
    "        \n",
    "    test_loss_history.append(test_loss.item())\n",
    "    print(f'Test loss: {test_loss: .3f}. \\\n",
    "            Test acc: {test_running_right / test_running_total: .5f}')\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4914633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
